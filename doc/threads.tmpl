            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Ahmed Afefy <es-AhmedAfifi2025@alexu.edu.eg>
Ziad Elbouriby <es-ziadmohamed2025@alexu.edu.eg>
Omar Ramadan <es-omarramadan2025@alexu.edu.eg>
Abdalla Milad <es_abdullah.melad2025@alexu.edu.eg>
Ziad Eslam <es_zeyad.saber2025@alexu.edu.eg>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int donated_priority;               /* Priority to be donated. */
    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

    int64_t wait_ticks;                   /* Number of ticks to be waiting for*/
    struct list waiters;                  /* List of locks held by this thread */
    struct lock *lock_waiting;            /* Lock this thread is waiting for. */
    struct condition *wait_cond;          /* Condition the thread is waiting for. */
    struct semaphore_elem *wait_cond_elem;/* Semaphore element for the thread waiting for condition. */

    struct list *sem_list;

    int niceness;
    real_t recent_cpu;

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.
void
timer_sleep (int64_t ticks) 
{
  if (ticks <= 0)
    return;
    
  ASSERT (intr_get_level () == INTR_ON);
  enum intr_level old_level = intr_disable ();
  thread_sleep(timer_ticks() + ticks);
  intr_set_level(old_level);
}

void
thread_sleep (int64_t local_thread_ticks)
{
  ASSERT(intr_get_level() == INTR_OFF);
  struct thread *t = thread_current();
  t->wait_ticks = local_thread_ticks;
  list_insert_ordered(&blocked_list, &t->elem, &thread_sleep_compare, NULL);
  thread_block();
}
When calling timer_sleep(int64_t ticks)function it had a busy waiting so it is
removed and replaced with another function thread_sleep (int64_t local_thread_ticks)
when a thread is put to sleep for a given number of ticks.
When calling thread_sleep (int64_t local_thread_ticks), the thread is
inserted in descending order with respect to its priority into blocked_list. Hence,
if two threads have wait_ticks set to zero, the one with the highest priority is unblocked first.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
static void
timer_interrupt (struct intr_frame *args UNUSED)
{
  ticks++;

  if(thread_mlfqs == BSD_SCHEDULER)
  {
    threads_update_statistics((timer_ticks() % TIMER_FREQ) == 0);

    if(timer_ticks() % 4 == 0)
      bsd_recalc_priority();
  }

  is_time_slice_end();
  thread_awake(ticks);
  thread_tick ();
}

void
thread_awake(int64_t ticks)
{
  struct thread *t;
  int max_priority = PRI_MIN;
  while (!list_empty(&blocked_list) && (t = list_entry(list_front(&blocked_list), struct thread, elem)) && t->wait_ticks <= ticks) {
    list_remove(&t->elem);
    thread_unblock(t);
    max_priority = max(max_priority, (t->donated_priority != -1? t->donated_priority: t->priority));
  }
  if(max_priority > thread_get_priority())
    intr_yield_on_return();
}

- The handler increments the tick count (ticks) at the beginning of the interrupt. This operation is likely optimized for efficiency,
as it involves a simple arithmetic operation (ticks++) without significant overhead.
- Certain operations within the interrupt handler, such as updating statistics and recalculating priorities, are conditionally executed
based on the scheduling policy (thread_mlfqs == BSD_SCHEDULER). This conditional execution ensures that potentially time-consuming tasks
are performed only when necessary, minimizing overhead during each interrupt.
- Priority recalculation is performed periodically (timer_ticks() % 4 == 0), reducing the frequency of this potentially expensive operation.
By recalculating priorities at intervals, rather than on every tick, the handler reduces the computational burden and improves efficiency.
- The thread_awake() function is invoked within the interrupt handler to check if any threads are ready to be awakened based on their
wait time (wait_ticks). This operation iterates over the blocked_list and unblocks threads whose wait time has elapsed, potentially reducing
the number of threads waiting to be unblocked during subsequent interrupts.
- After unblocking threads, the handler compares the maximum priority of the unblocked threads with the priority of the currently running 
thread (thread_get_priority()). If the unblocked threads have higher priority, the handler schedules a yield operation (intr_yield_on_return()),
allowing higher-priority threads to preempt the current thread efficiently.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
void
timer_sleep (int64_t ticks) 
{
  if (ticks <= 0)
    return;
    
  ASSERT (intr_get_level () == INTR_ON);
  enum intr_level old_level = intr_disable ();
  thread_sleep(timer_ticks() + ticks);
  intr_set_level(old_level);
}
List operations happen during interrupt is disabled. 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
void
timer_sleep (int64_t ticks) 
{
  if (ticks <= 0)
    return;
    
  ASSERT (intr_get_level () == INTR_ON);
  enum intr_level old_level = intr_disable ();
  thread_sleep(timer_ticks() + ticks);
  intr_set_level(old_level);
}
With the interrupt disabled during the thread operation, this function is almost
atomic.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
A sleep list was chosen to manage sleeping threads due to its straightforward 
implementation and alignment with the existing design pattern of the ready list.
This choice simplifies the implementation process and maintains consistency within
the system's architecture.
Opting for a sorted list and inserting threads based on wake-up time order enhances
efficiency, particularly in scenarios with numerous sleeping threads. This design
choice facilitates quicker retrieval of threads ready to be awakened, thereby improving
overall system performance.
The sleep list is treated as an external static variable, restricting its access to
designated functions, and promoting safety and maintainability by minimizing the risk of
unintended modifications or errors.
While alternative designs were not considered, this approach is believed to be both safe
and reasonable, offering an efficient solution to manage sleeping threads.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int donated_priority;               /* Priority to be donated. */
    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

    int64_t wait_ticks;                   /* Number of ticks to be waiting for*/
    struct list waiters;                  /* List of locks held by this thread */
    struct lock *lock_waiting;            /* Lock this thread is waiting for. */
    struct condition *wait_cond;          /* Condition the thread is waiting for. */
    struct semaphore_elem *wait_cond_elem;/* Semaphore element for the thread waiting for condition. */

    struct list *sem_list;

    int niceness;
    real_t recent_cpu;

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };

In synch.h
struct semaphore_elem 
  {
    struct list_elem elem;              /* List element. */
    struct semaphore semaphore;         /* This semaphore. */
    struct thread* waiting_thread;      /* Waiting thread. */
  };
struct lock 
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    struct list_elem lockelem;  /* List element for the list of locks held by a thread */
    int donated_priority;       /* Lock donated priority. */
  };


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
Priority donation is tracked using nested locks where each thread can hold
a list of locks. Each lock keeps track of its own donated priority. 
Threads waiting on a lock can donate their priority to the lock holder,
and this donation can propagate through a chain of locks.

Thread A (priority 50) -> waiting on Lock X
Thread B (priority 40) -> holds Lock X, waiting on Lock Y
Thread C (priority 30) -> holds Lock Y, waiting on Lock Z
Thread D (priority 20) -> holds Lock Z

When Thread A attempts to acquire Lock X, Thread B’s priority
is temporarily raised to 50. This donation continues through the
chain of locks, ultimately raising Thread D’s effective priority
to 50 until Thread A no longer needs Lock X.
Thread A -> Lock X -> Thread B -> Lock Y -> Thread C -> Lock Z -> Thread D

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?
To ensure the highest priority thread wakes up first:
- Semaphores: Use list_insert_ordered to insert threads into the semaphore's
waiters list based on priority.
- Locks: When a lock is released, the semaphore's waiters list is checked, and
the highest priority thread is unblocked first.
- Condition Variables: Use list_insert_ordered for condition variables to 
maintain the waiters list in priority order.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
- Thread T1 (high priority) attempts to acquire a lock held by Thread T2 (lower priority).
- Thread T1 donates its priority to Thread T2.
- Thread T2 is now effectively running at Thread T1's priority.
- If Thread T2 is also waiting on another lock held by Thread T3, the donation propagates to Thread T3.
- This continues until the chain is resolved, ensuring the highest priority thread is not unduly delayed.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
- Thread T1 releases a lock.
- The lock’s priority donation is reset.
- Thread T1 recalculates its donated priority based on other locks it holds.
- The highest priority thread from the semaphore’s waiters list is unblocked.
- The unblocked thread preempts if its priority is higher than the current running thread.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
# Race condition
- When changing the priority of a running thread, another thread might simultaneously donate
priority due to lock acquisition.

# Avoidance
- Disabled interrupts: Ensure atomic access to thread priority.
- Avoid locks: Using locks inside thread_set_priority can lead to deadlocks, so disabling interrupts is a safer approach.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
# Chosen Design
- Priority donation through nested locks with lists tracking locks and priorities.
- Use of ordered lists for semaphores and condition variables to manage waiting threads.

# Superiority
- Efficiency: Minimizes context switching and ensures highest priority tasks are addressed promptly.
- Simplicity: Using list operations to manage priorities is straightforward and integrates well with existing Pintos structures.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     C
28     16   8   4  59  59  58     B
32     16  12   4  59  58  58     A
36     20  12   4  58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?
# Ambiguity
- How frequently priorities should be recalculated.

# Resolution
- Priorities are recalculated based on the system tick frequency, aligned 
with the given rules in the Pintos project documentation.


>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
# Interrupt context
- Scheduling decisions are made to ensure real-time responsiveness.
- Frequent context switches in interrupt context could degrade performance.

# Non-interrupt context
- Heavy computations and updates to data structures are handled here to avoid 
slowing down interrupt handling.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?
# Advantages:
- Responsiveness: Ensures high-priority tasks are not delayed.
- Efficiency: Limits the scope of changes to priority donation paths.

# Disadvantages:
- Complexity: Priority donation and nested donation logic can be intricate.
- Overhead: Maintaining ordered lists and recalculating priorities adds 
computational overhead.

# Improvements:
- Refine Priority Calculation: Optimize the frequency and method of recalculating
priorities to balance load.
- Simplify Data Structures: Investigate alternate data structures that may simplify
the logic while maintaining efficiency.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?
# Implementation Approach
The fixed-point arithmetic implementation using an abstract data type and inline
functions balances performance and maintainability, ensuring clean, readable, and
modifiable scheduling logic in Pintos.
The fixed-point arithmetic was implemented using an abstract data type (real_t) and
a set of inline functions to perform arithmetic operations. This approach offers several
advantages:
- Encapsulation and abstraction
  * The real_t structure encapsulates the fixed-point representation, hiding the 
  implementation details from the rest of the code.
  * Functions like to_real, to_int, and round_real provide a clear interface for converting
  between integers and fixed-point numbers.
- Inline Functions for Performance:
  * Using static inline functions ensures that the arithmetic operations are inlined by the compiler,
  eliminating function call overhead.
  * Inline functions allow type checking and prevent the pitfalls of using macros.
- Comprehensive Set of Operations:
  * The set of functions covers all necessary arithmetic operations, including addition, subtraction, 
  multiplication, and division of fixed-point numbers, as well as operations involving integers.

# Why an Abstraction Layer?
  - Simplified Code Maintenance:
    * With a clear abstraction layer, any changes to the fixed-point arithmetic implementation can be 
    made in one place.
    * The abstraction layer ensures consistent arithmetic rules across the codebase.
  - Enhanced Readability and Debugging:
    * Named functions make the purpose of each operation explicit, improving code readability and ease 
    of debugging.
    * Descriptive function names clarify the intention behind each calculation.
  - Flexibility and Extensibility:
    * The abstraction layer makes it easy to extend the fixed-point arithmetic library with additional
    operations if needed.
    * Performance optimizations or additional features can be added to the fixed-point functions without
    affecting the rest of the scheduler code.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
